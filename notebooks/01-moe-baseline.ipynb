{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==================================================\n# Imports and setup\n# ==================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport time\nfrom collections import defaultdict\n\n# Set style for better plots\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 4)\n\n# Set random seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\nprint(\"=\"*60)\nprint(\"BASELINE MLP IMPLEMENTATION\")\nprint(\"=\"*60)\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(\"=\"*60)","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# Data Loading\n# ==================================================\n\ndef load_mnist(batch_size=64, data_dir='/kaggle/working/data'):\n    \"\"\"\n    Load MNIST with proper normalization \n\n    Args: \n        batch_size: Batch size for training -> 64\n        data_dir: where to download MNIST data\n\n    Returns:\n        train_loader: DatalLoader for training\n        test_loader: DataLoader for testing\n    \"\"\"\n    print(\"\\n Loading MNIST Dataset\")\n\n    # MNIST normalization values (mean=0.1307, std=0.3081)\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    # Download and load training data\n    train_dataset = datasets.MNIST(\n        root=data_dir,\n        train=True,\n        download=True,\n        transform=transform\n    )\n\n    # Download and load test data\n    test_dataset = datasets.MNIST(\n        root=data_dir,\n        train=False,\n        download=True,\n        transform=transform\n    )\n\n    # Create data loaders (pin_memory=True for faster GPU transfers)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    print(f\"Tranining samples: {len(train_dataset):,}\")\n    print(f\"Test samples: {len(test_dataset):,}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Training batches: {len(train_loader)}\")\n\n    return train_loader, test_loader","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# Baseline MLP Model\n# ==================================================\n\nclass BaselineMLP(nn.Module):\n    \"\"\"\n    Simple 2-layer MLP for MNIST classification\n\n    This is the performance baseline - Moe should match or beat this\n\n    Architechture:\n        Input (784) -> FC1(128) -> ReLU -> FC2(64) -> ReLU -> FC3(10)\n    \"\"\"\n    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):\n        super(BaselineMLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n        self.relu = nn.ReLU()\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        \"\"\"\n        Xavier initialization for better training\n        \"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass\n\n        Args: \n            x: Input tensor (batch, 28, 28) or (batch, 784)\n\n        Returns: \n            Logits for 10 classes (batch, 10)\n        \"\"\"\n        # Flatten image (batch, 28, 28) -> (batch, 784)\n        if x.dim() > 2:\n            x = x.view(x.size(0), -1)\n\n        # Forward through the layers\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        return x\n\n    def count_parameters(self):\n        \"\"\"\n        Count trainable parameters\n        \"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================================================\n# Training Function \n# ==================================================\n\ndef train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n    \"\"\"\n    Train for one epoch\n\n    Args:\n        model: The neural network\n        train_loader: Training data\n        optimizer: Optimizer (Adam)\n        criterion: Loss function (CrossEntropy)\n        device: 'cuda' or 'cpu'\n        epoch: Current epoch number\n\n    Returns: \n        avg_loss: Average training loss for the epoch \n    \"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Progress bar\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n\n    for batch_idx, (data, target) in enumerate(pbar):\n        # Move to GPU\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += (pred == target).sum().item()\n        total += target.size(0)\n\n        # Update progress bar\n        pbar.set_postfix({\n            'loss': f'{loss.item():.4f}',\n            'acc': f'{100.*correct/total:.2f}%'\n        })\n\n    avg_loss = total_loss / len(train_loader)\n    train_acc = 100. * correct / total\n\n    return avg_loss, train_acc\n\ndef evaluate(model, test_loader, criterion, device):\n    \"\"\"\n    Evaluate model on test set\n\n    Args:\n        model: The neural network\n        test_loader: Test data\n        criterion: Loss function\n        device: 'cuda' or 'cpu'\n\n    Returns:\n        test_loss: Average test loss\n        test_acc: Test accuracy(%)\n    \"\"\"\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n\n            output = model(data)\n            test_loss += criterion(output, target).item()\n\n            pred = output.argmax(dim=1)\n            correct += (pred == target).sum().item()\n            total += target.size(0)\n\n    test_loss /= len(test_loader)\n    test_acc = 100.*correct / total\n\n    return test_loss, test_acc\n\ndef train_baseline(model, train_loader, test_loader, epochs=10, lr=0.001, device='cuda'):\n    \"\"\"\n    Complete training loop\n\n    Args: \n        model: BaselineMLP instance\n        train_loader: Training data\n        test_loader: Test data\n        epochs: Number of epochs\n        lr: Learning rate\n        device: 'cuda' or 'cpu'\n\n    Returns:\n        history: Dictionary with training metrics\n    \"\"\"\n    print(f\"STARTING TRAINING...\")\n    print(f\"Model parameters: {model.count_parameters():,}\")\n    print(f\"Device: {device}\")\n    print(f\"Epochs: {epochs}\")\n    print(f\"Learning rate: {lr}\")\n    print(\"-\" * 60)\n\n    # Setup\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Move model to GPU\n    model = model.to(device)\n    \n    # Track history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'test_loss': [],\n        'test_acc': [],\n        'epoch_times': []\n    }\n    \n    # Training loop\n    best_acc = 0\n    for epoch in range(1, epochs + 1):\n        start_time = time.time()\n        \n        # Train\n        train_loss, train_acc = train_epoch(\n            model, train_loader, optimizer, criterion, device, epoch\n        )\n        \n        # Evaluate\n        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n        \n        # Track time\n        epoch_time = time.time() - start_time\n        \n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['test_loss'].append(test_loss)\n        history['test_acc'].append(test_acc)\n        history['epoch_times'].append(epoch_time)\n        \n        # Print summary\n        print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n        print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n        print(f\"   Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n        print(f\"   Time: {epoch_time:.1f}s\")\n        \n        # Save best model\n        if test_acc > best_acc:\n            best_acc = test_acc\n            print(f\"New best accuracy!!!\")\n        \n        print(\"-\" * 60)\n    \n    # Final summary\n    print(\"\\nTraining Complete!!!\")\n    print(f\"Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n    print(f\"Target: 98%\")\n    \n    if history['test_acc'][-1] >= 97:\n        print(\"SUCCESS: Baseline achieved target!\")\n    else:\n        print(\"Need improvement: Try training longer or adjusting hyperparameters\")\n    \n    return history","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================    \n# VISUALIZATION\n# ================================================================\n\ndef plot_training_curves(history, save_path='/kaggle/working/baseline_results.png'):\n    \"\"\"\n    Create comprehensive training visualization.\n    \n    Args:\n        history: Training history dictionary\n        save_path: Where to save the plot\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Plot 1: Loss curves\n    axes[0].plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\n    axes[0].plot(epochs, history['test_loss'], 'r-s', label='Test Loss', linewidth=2, markersize=6)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title('Training & Test Loss', fontsize=14, fontweight='bold')\n    axes[0].legend(fontsize=11)\n    axes[0].grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy curves\n    axes[1].plot(epochs, history['train_acc'], 'b-o', label='Train Acc', linewidth=2, markersize=6)\n    axes[1].plot(epochs, history['test_acc'], 'r-s', label='Test Acc', linewidth=2, markersize=6)\n    axes[1].axhline(y=98, color='green', linestyle='--', label='Target (98%)', alpha=0.7, linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n    axes[1].set_title('Training & Test Accuracy', fontsize=14, fontweight='bold')\n    axes[1].legend(fontsize=11)\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_ylim([90, 100])\n    \n    # Plot 3: Training time per epoch\n    axes[2].bar(epochs, history['epoch_times'], color='skyblue', alpha=0.7, edgecolor='navy')\n    axes[2].set_xlabel('Epoch', fontsize=12)\n    axes[2].set_ylabel('Time (seconds)', fontsize=12)\n    axes[2].set_title('Training Time Per Epoch', fontsize=14, fontweight='bold')\n    axes[2].grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    print(f\"\\nPlot saved to: {save_path}\")\n    plt.show()\n\n\ndef print_final_summary(history):\n    \"\"\"Print comprehensive summary of training.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"BASELINE MLP - FINAL SUMMARY\")\n    print(\"=\"*60)\n    \n    print(f\"\\nPerformance Metrics:\")\n    print(f\"   Final Test Accuracy:  {history['test_acc'][-1]:.2f}%\")\n    print(f\"   Best Test Accuracy:   {max(history['test_acc']):.2f}%\")\n    print(f\"   Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")\n    print(f\"   Final Test Loss:      {history['test_loss'][-1]:.4f}\")\n    \n    print(f\"\\nTraining Time:\")\n    print(f\"   Total time:           {sum(history['epoch_times']):.1f}s\")\n    print(f\"   Avg time per epoch:   {np.mean(history['epoch_times']):.1f}s\")\n    \n    print(f\"\\nDay 9 Checklist:\")\n    print(f\"   [{'✓' if history['test_acc'][-1] >= 97 else '✗'}] Accuracy >= 97%\")\n    print(f\"   [✓] Model implemented\")\n    print(f\"   [✓] Training pipeline working\")\n    print(f\"   [✓] Visualization created\")\n    \n    print(\"\\n\" + \"=\"*60)","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# MAIN EXECUTION\n# ================================================================\n\n# Configuration\nBATCH_SIZE = 64\nEPOCHS = 10\nLEARNING_RATE = 0.001\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Step 1: Load data\ntrain_loader, test_loader = load_mnist(batch_size=BATCH_SIZE)\n\n# Step 2: Create model\nprint(\"\\nBuilding Model...\")\nmodel = BaselineMLP(input_dim=784, hidden_dim=128, output_dim=10)\nprint(f\"✓ Model created with {model.count_parameters():,} parameters\")\n\n# Step 3: Train model\nhistory = train_baseline(\n    model=model,\n    train_loader=train_loader,\n    test_loader=test_loader,\n    epochs=EPOCHS,\n    lr=LEARNING_RATE,\n    device=DEVICE\n)\n\n# Step 4: Visualize results\nplot_training_curves(history)\n\n# Step 5: Print final summary\nprint_final_summary(history)\n\n# Step 6: Save model\nmodel_path = '/kaggle/working/baseline_mlp.pth'\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'history': history,\n    'config': {\n        'input_dim': 784,\n        'hidden_dim': 128,\n        'output_dim': 10,\n        'epochs': EPOCHS,\n        'lr': LEARNING_RATE\n    }\n}, model_path)\nprint(f\"\\nModel saved to: {model_path}\")","metadata":{"_uuid":"ce48d070-785d-447b-9c2f-d4b772c8df80","_cell_guid":"24ccda74-97b8-4ae7-a96a-827a1c745489","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-10T06:13:06.799808Z","iopub.execute_input":"2026-02-10T06:13:06.800534Z","iopub.status.idle":"2026-02-10T06:15:55.416211Z","shell.execute_reply.started":"2026-02-10T06:13:06.800502Z","shell.execute_reply":"2026-02-10T06:15:55.415446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"f32340c7-2c2f-4ef6-ab0d-baa0bf99d5a5","_cell_guid":"fa3ebe84-728d-419b-96c4-dd86be2a7414","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}